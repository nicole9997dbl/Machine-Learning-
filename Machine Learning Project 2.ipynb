{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Project 2\n",
    "\n",
    "Project Description:\n",
    "- Use same datasets as Project 1.\n",
    "- Preprocess data: Explore data and apply data scaling.\n",
    "\n",
    "Regression Task:\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 1 again on data you get from PCA. Compare your results with results in project 2. You don't need to apply all the models twice. Just copy the result table from project 1, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Classification Task:\n",
    "- Apply two voting classifiers - one with hard voting and one with soft voting\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 1 again on data you get from PCA. Compare your results with results in project 1. You don't need to apply all the models twice. Just copy the result table from project 1, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Deliverables:\n",
    "- Use markdown to provide inline comments for this project.\n",
    "- Your outputs should be clearly executed in the notebook i.e. we should not need to rerun the code to obtain the outputs.\n",
    "- Visualization encouraged.\n",
    "- If you are submitting two different files, then please only one group member submit both the files. If you submit two files separately from different accounts, it will be submitted as two different attempts.\n",
    "- If you are submitting two different files, then please follow below naming convetion:\n",
    "    Project2_Regression_GroupXX_Firstname1_Firstname2.ipynb\n",
    "    Project2_Classification_GroupXX_Firstname1_Firstname2.ipynb\n",
    "- If you are submitting single file, then please follow below naming convetion:\n",
    "    Project2_Both_GroupXX_Firstname1_Firstname2.ipynb\n",
    "\n",
    "Questions regarding the project:\n",
    "- We have created a discussion board under Projects folder on e-learning. Create threads over there and post your queries related to project there.\n",
    "- We will also answer queries there. We will not be answering any project related queries through the mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_risk = pd.read_csv('audit_risk.csv')\n",
    "trial = pd.read_csv('trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(audit_risk, trial, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sector_score      float64\n",
       "LOCATION_ID        object\n",
       "PARA_A            float64\n",
       "Score_A           float64\n",
       "Risk_A            float64\n",
       "PARA_B            float64\n",
       "Score_B           float64\n",
       "Risk_B            float64\n",
       "TOTAL             float64\n",
       "numbers           float64\n",
       "Score_B.1         float64\n",
       "Risk_C            float64\n",
       "Money_Value       float64\n",
       "Score_MV          float64\n",
       "Risk_D            float64\n",
       "District_Loss       int64\n",
       "PROB              float64\n",
       "RiSk_E            float64\n",
       "History             int64\n",
       "Prob              float64\n",
       "Risk_F            float64\n",
       "Score             float64\n",
       "Inherent_Risk     float64\n",
       "CONTROL_RISK      float64\n",
       "Detection_Risk    float64\n",
       "Audit_Risk        float64\n",
       "Risk                int64\n",
       "SCORE_A           float64\n",
       "SCORE_B           float64\n",
       "Marks             float64\n",
       "MONEY_Marks       float64\n",
       "District          float64\n",
       "Loss              float64\n",
       "LOSS_SCORE        float64\n",
       "History_score     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the records with non-numerical values and convert the LOCATION_ID into numerical data\n",
    "df = df[df.LOCATION_ID.apply(lambda x: x.isnumeric())]\n",
    "df.LOCATION_ID = df.LOCATION_ID.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score</th>\n",
       "      <th>LOCATION_ID</th>\n",
       "      <th>PARA_A</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>numbers</th>\n",
       "      <th>...</th>\n",
       "      <th>Audit_Risk</th>\n",
       "      <th>Risk</th>\n",
       "      <th>SCORE_A</th>\n",
       "      <th>SCORE_B</th>\n",
       "      <th>Marks</th>\n",
       "      <th>MONEY_Marks</th>\n",
       "      <th>District</th>\n",
       "      <th>Loss</th>\n",
       "      <th>LOSS_SCORE</th>\n",
       "      <th>History_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>23</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.508</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7148</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.966</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3096</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.480</td>\n",
       "      <td>10.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5060</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2832</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sector_score  LOCATION_ID  PARA_A  Score_A  Risk_A  PARA_B  Score_B  \\\n",
       "0          3.89           23    4.18      0.6   2.508    2.50      0.2   \n",
       "1          3.89            6    0.00      0.2   0.000    4.83      0.2   \n",
       "2          3.89            6    0.51      0.2   0.102    0.23      0.2   \n",
       "3          3.89            6    0.00      0.2   0.000   10.80      0.6   \n",
       "4          3.89            6    0.00      0.2   0.000    0.08      0.2   \n",
       "\n",
       "   Risk_B  TOTAL  numbers      ...        Audit_Risk  Risk  SCORE_A  SCORE_B  \\\n",
       "0   0.500   6.68      5.0      ...            1.7148     1      6.0      2.0   \n",
       "1   0.966   4.83      5.0      ...            0.5108     0      2.0      2.0   \n",
       "2   0.046   0.74      5.0      ...            0.3096     0      2.0      2.0   \n",
       "3   6.480  10.80      6.0      ...            3.5060     1      2.0      6.0   \n",
       "4   0.016   0.08      5.0      ...            0.2832     0      2.0      2.0   \n",
       "\n",
       "   Marks  MONEY_Marks  District  Loss  LOSS_SCORE  History_score  \n",
       "0    2.0          2.0       2.0   0.0         2.0            2.0  \n",
       "1    2.0          2.0       2.0   0.0         2.0            2.0  \n",
       "2    2.0          2.0       2.0   0.0         2.0            2.0  \n",
       "3    6.0          6.0       2.0   0.0         2.0            2.0  \n",
       "4    2.0          2.0       2.0   0.0         2.0            2.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score_A and Score_B contain the same information with SCORE_A and SCORE_B while the two pairs are on a different scale. So we\n",
    "# keep SCORE_A and SCORE_B and delete Score_A and Score_B, in this way, we avoid data redundancy and also keep the two columns \n",
    "# in the same scale.\n",
    "df.drop(['Score_A', 'Score_B'], axis=1, inplace = True)\n",
    "df.dropna(axis = 0, inplace = True) # Here we drop the data with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(625, 33)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sector_score      float64\n",
       "LOCATION_ID         int64\n",
       "PARA_A            float64\n",
       "Risk_A            float64\n",
       "PARA_B            float64\n",
       "Risk_B            float64\n",
       "TOTAL             float64\n",
       "numbers           float64\n",
       "Score_B.1         float64\n",
       "Risk_C            float64\n",
       "Money_Value       float64\n",
       "Score_MV          float64\n",
       "Risk_D            float64\n",
       "District_Loss       int64\n",
       "PROB              float64\n",
       "RiSk_E            float64\n",
       "History             int64\n",
       "Prob              float64\n",
       "Risk_F            float64\n",
       "Score             float64\n",
       "Inherent_Risk     float64\n",
       "CONTROL_RISK      float64\n",
       "Detection_Risk    float64\n",
       "Audit_Risk        float64\n",
       "Risk                int64\n",
       "SCORE_A           float64\n",
       "SCORE_B           float64\n",
       "Marks             float64\n",
       "MONEY_Marks       float64\n",
       "District          float64\n",
       "Loss              float64\n",
       "LOSS_SCORE        float64\n",
       "History_score     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Audit_Risk', 'Risk'], axis = 1)\n",
    "y = df['Audit_Risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting on Linear Regression and Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 200}\n",
      "Best cross-validation score: -4401051843858142.50\n"
     ]
    }
   ],
   "source": [
    "# Bagging on Linear Regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "lreg = LinearRegression()\n",
    "bag_lreg = BaggingRegressor(lreg, bootstrap=True, random_state=0)\n",
    "grid_search = GridSearchCV(bag_lreg, param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for bagging linear regression with best parameters: 0.6044707375465956\n"
     ]
    }
   ],
   "source": [
    "bag_lreg1 = BaggingRegressor(lreg, n_estimators=200, bootstrap=True, random_state=0)\n",
    "bag_lreg1.fit(X_train, y_train)\n",
    "y_test_pred = bag_lreg1.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R_squred score for bagging linear regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Bagging on Polynomial Regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2) # From project1 we know the best parameter for polynomial regression is n=2\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "lreg = LinearRegression()\n",
    "bag_lreg = BaggingRegressor(lreg, bootstrap=True, random_state=0)\n",
    "grid_search = GridSearchCV(bag_lreg, param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for bagging polynomial regression with best parameters: 0.9978896899820212\n"
     ]
    }
   ],
   "source": [
    "bag_poly1 = BaggingRegressor(lreg, n_estimators=100, bootstrap=True, random_state=0)\n",
    "bag_poly1.fit(X_train_poly, y_train)\n",
    "y_test_pred = bag_poly1.predict(X_test_poly)\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R_squred score for bagging polynomial regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 400}\n",
      "Best cross-validation score: -72899493833805.97\n"
     ]
    }
   ],
   "source": [
    "# Pasting on Linear Regression\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "lreg = LinearRegression()\n",
    "past_lreg = BaggingRegressor(lreg, bootstrap=False, random_state=0)\n",
    "grid_search = GridSearchCV(past_lreg, param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for pasting linear regression with best parameters: 0.4437886917390246\n"
     ]
    }
   ],
   "source": [
    "past_lreg1 = BaggingRegressor(lreg, n_estimators=400, bootstrap=False, random_state=0)\n",
    "past_lreg1.fit(X_train, y_train)\n",
    "y_test_pred = past_lreg1.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R_squred score for pasting linear regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 200}\n",
      "Best cross-validation score: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Pasting on Polynomial Regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2) # From project1 we know the best parameter for polynomial regression is n=2\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "lreg = LinearRegression()\n",
    "bag_lreg = BaggingRegressor(lreg, bootstrap=False, random_state=0)\n",
    "grid_search = GridSearchCV(bag_lreg, param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for pasting polynomial regression with best parameters: 0.9992841739114662\n"
     ]
    }
   ],
   "source": [
    "bag_poly1 = BaggingRegressor(lreg, n_estimators=200, bootstrap=False, random_state=0)\n",
    "bag_poly1.fit(X_train_poly, y_train)\n",
    "y_test_pred = bag_poly1.predict(X_test_poly)\n",
    "\n",
    "print(\"R_squred score for pasting polynomial regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost boosting on Linear Regression and Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Adaboost on linear regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "lreg = LinearRegression()\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "ada_reg = AdaBoostRegressor(lreg, random_state=0)\n",
    "grid_search = GridSearchCV(ada_reg, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for adaboost linear regression with best parameters: 0.31808843737881387\n"
     ]
    }
   ],
   "source": [
    "ada_reg1 = AdaBoostRegressor(lreg, n_estimators=100, learning_rate=0.2, random_state=0)\n",
    "ada_reg1.fit(X_train, y_train)\n",
    "y_test_pred = ada_reg1.predict(X_test)\n",
    "print(\"R_squred score for adaboost linear regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.4, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Adaboost on polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2) # From project1 we know the best parameter for polynomial regression is n=2\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "ada_reg = AdaBoostRegressor(lreg, random_state=0)\n",
    "grid_search = GridSearchCV(ada_reg, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for adaboost polynomial regression with best parameters: 0.9990410846224799\n"
     ]
    }
   ],
   "source": [
    "ada_reg1 = AdaBoostRegressor(lreg, n_estimators=100, learning_rate=0.4, random_state=0)\n",
    "ada_reg1.fit(X_train_poly, y_train)\n",
    "y_test_pred = ada_reg1.predict(X_test_poly)\n",
    "print(\"R_squred score for adaboost polynomial regression with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model(Decision Tree Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'n_estimators': 500}\n",
      "Best cross-validation score: 0.76\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.ensemble import GradientBoostingRegressor\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "grid_search = GridSearchCV(gbrt, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for gradient boosting model with best parameters: 0.8980581256067349\n"
     ]
    }
   ],
   "source": [
    "gbrt_reg1 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.2, random_state=0)\n",
    "gbrt_reg1.fit(X_train, y_train)\n",
    "y_test_pred = gbrt_reg1.predict(X_test)\n",
    "print(\"R_squred score for gradient boosting model with best parameters: {}\".format(r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Regressions with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_components': 10}\n",
      "Best cross-validation score: 35.95\n"
     ]
    }
   ],
   "source": [
    "# Using grid search to find the best parameter for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "param = {\"n_components\": list(range(10, 21))}\n",
    "pca = PCA()\n",
    "grid_search = GridSearchCV(pca, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879864309421829"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca1 = PCA(n_components=10)\n",
    "pca1.fit(X_train, y_train)\n",
    "pca1.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = pca1.fit_transform(X_train)\n",
    "X_test_reduced = pca1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for KNN Regressor after PCA: 0.5638600866532306\n"
     ]
    }
   ],
   "source": [
    "# KNN regression after PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_reg3 = KNeighborsRegressor(3)\n",
    "knn_reg3.fit(X_train_reduced, y_train)\n",
    "y_pred = knn_reg3.predict(X_test_reduced)\n",
    "print(\"R_squred score for KNN Regressor after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for Linear Regression after PCA: 0.6812828438902796\n"
     ]
    }
   ],
   "source": [
    "# Linear regression after PCA\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train_reduced, y_train)\n",
    "y_pred = lreg.predict(X_test_reduced)\n",
    "print(\"R_squred score for Linear Regression after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for Ridge Regression after PCA: 0.6798978468810819\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression after PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha = 10) # From project1 we know the best parameter for ridge regression is alpha = 10.\n",
    "ridge.fit(X_train_reduced, y_train)\n",
    "y_pred = ridge.predict(X_test_reduced)\n",
    "print(\"R_squred score for Ridge Regression after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for Linear Regression after PCA: 0.6812828438902796\n"
     ]
    }
   ],
   "source": [
    "# LASSO regression after PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha = 1) # From project1 we know the best parameter for lasso regression is alpha = 1.\n",
    "lasso.fit(X_train_reduced, y_train)\n",
    "y_pred = lreg.predict(X_test_reduced)\n",
    "print(\"R_squred score for Linear Regression after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for Polynomial Regression after PCA: -0.027996933737006735\n"
     ]
    }
   ],
   "source": [
    "# Polynomial regresssion after PCA\n",
    "poly = PolynomialFeatures(2) # From project1 we know the best parameter for polynomial regression is n=2\n",
    "X_train_poly_reduced = poly.fit_transform(X_train_reduced)\n",
    "X_test_poly_reduced = poly.transform(X_test_reduced)\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train_poly_reduced, y_train)\n",
    "y_pred = lreg.predict(X_test_poly_reduced)\n",
    "print(\"R_squred score for Polynomial Regression after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for LinearSVR after PCA: 0.6794504612592609\n"
     ]
    }
   ],
   "source": [
    "# LinearSVR/SVR without kernel after PCA\n",
    "from sklearn.svm import LinearSVR\n",
    "lsvr = LinearSVR(C=100) # From project1 we know the best parameter for linear SVR is C=100.\n",
    "lsvr.fit(X_train_reduced, y_train)\n",
    "y_pred = lsvr.predict(X_test_reduced)\n",
    "print(\"R_squred score for LinearSVR after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for SVR with rbf kernel after PCA: 0.7782835132806085\n"
     ]
    }
   ],
   "source": [
    "# SVR with rbf kernel after PCA\n",
    "from sklearn.svm import SVR\n",
    "svr_rbf = SVR(kernel='rbf',C=100, gamma=0.1) # From project1 we know the best parameter for SVR with rbf kernel is C=100, gamma=0.1.\n",
    "svr_rbf.fit(X_train_reduced, y_train)\n",
    "y_pred = svr_rbf.predict(X_test_reduced)\n",
    "print(\"R_squred score for SVR with rbf kernel after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for SVR with poly kernel after PCA: -0.4591227911747484\n"
     ]
    }
   ],
   "source": [
    "# SVR with poly kernel after PCA\n",
    "svr_poly = SVR(kernel=\"poly\",C=1, gamma=10) # From project1 we know the best parameter for SVR with poly kernel is C=1, gamma=10.\n",
    "svr_poly.fit(X_train_reduced, y_train)\n",
    "y_pred = svr_poly.predict(X_test_reduced)\n",
    "print(\"R_squred score for SVR with poly kernel after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred score for SVR with poly kernel after PCA: -0.10432458430378411\n"
     ]
    }
   ],
   "source": [
    "# SVR with linear kernel after PCA\n",
    "svr_poly = SVR(kernel=\"poly\",C=100, gamma=0.001) # From project1 we know the best parameter for SVR with poly kernel is C=100, gamma=0.001.\n",
    "svr_poly.fit(X_train_reduced, y_train)\n",
    "y_pred = svr_poly.predict(X_test_reduced)\n",
    "print(\"R_squred score for SVR with poly kernel after PCA: {}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning: Simple Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 31)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "468/468 [==============================] - 0s 226us/sample - loss: 2407.8154 - mean_squared_error: 2407.8154\n",
      "Epoch 2/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2392.7546 - mean_squared_error: 2392.7546\n",
      "Epoch 3/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2380.0527 - mean_squared_error: 2380.0527\n",
      "Epoch 4/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2368.9661 - mean_squared_error: 2368.9661\n",
      "Epoch 5/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2359.0049 - mean_squared_error: 2359.0049\n",
      "Epoch 6/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2349.8599 - mean_squared_error: 2349.8599\n",
      "Epoch 7/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2341.3555 - mean_squared_error: 2341.3555\n",
      "Epoch 8/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2333.4033 - mean_squared_error: 2333.4033\n",
      "Epoch 9/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2325.9658 - mean_squared_error: 2325.9658\n",
      "Epoch 10/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2319.0305 - mean_squared_error: 2319.0305\n",
      "Epoch 11/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2312.5898 - mean_squared_error: 2312.5898\n",
      "Epoch 12/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2306.6313 - mean_squared_error: 2306.6313\n",
      "Epoch 13/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2301.1353 - mean_squared_error: 2301.1353\n",
      "Epoch 14/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2296.0720 - mean_squared_error: 2296.0720\n",
      "Epoch 15/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2291.4070 - mean_squared_error: 2291.4070\n",
      "Epoch 16/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2287.0974 - mean_squared_error: 2287.0974\n",
      "Epoch 17/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2283.0989 - mean_squared_error: 2283.0989\n",
      "Epoch 18/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2279.3643 - mean_squared_error: 2279.3643\n",
      "Epoch 19/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2275.8474 - mean_squared_error: 2275.8474\n",
      "Epoch 20/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2272.5049 - mean_squared_error: 2272.5049\n",
      "Epoch 21/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2269.2974 - mean_squared_error: 2269.2974\n",
      "Epoch 22/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2266.1887 - mean_squared_error: 2266.1887\n",
      "Epoch 23/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2263.1475 - mean_squared_error: 2263.1475\n",
      "Epoch 24/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2260.1482 - mean_squared_error: 2260.1482\n",
      "Epoch 25/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2257.1692 - mean_squared_error: 2257.1692\n",
      "Epoch 26/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2254.1936 - mean_squared_error: 2254.1936\n",
      "Epoch 27/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2251.2078 - mean_squared_error: 2251.2078\n",
      "Epoch 28/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2248.2014 - mean_squared_error: 2248.2014\n",
      "Epoch 29/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2245.1677 - mean_squared_error: 2245.1677\n",
      "Epoch 30/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2242.1023 - mean_squared_error: 2242.1023\n",
      "Epoch 31/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2239.0017 - mean_squared_error: 2239.0017\n",
      "Epoch 32/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2235.8640 - mean_squared_error: 2235.8640\n",
      "Epoch 33/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2232.6904 - mean_squared_error: 2232.6904\n",
      "Epoch 34/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2229.4814 - mean_squared_error: 2229.4814\n",
      "Epoch 35/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2226.2366 - mean_squared_error: 2226.2366\n",
      "Epoch 36/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2222.9597 - mean_squared_error: 2222.9597\n",
      "Epoch 37/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2219.6523 - mean_squared_error: 2219.6523\n",
      "Epoch 38/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2216.3157 - mean_squared_error: 2216.3157\n",
      "Epoch 39/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2212.9529 - mean_squared_error: 2212.9529\n",
      "Epoch 40/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2209.5659 - mean_squared_error: 2209.5659\n",
      "Epoch 41/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2206.1553 - mean_squared_error: 2206.1553\n",
      "Epoch 42/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2202.7258 - mean_squared_error: 2202.7258\n",
      "Epoch 43/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2199.2759 - mean_squared_error: 2199.2759\n",
      "Epoch 44/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2195.8093 - mean_squared_error: 2195.8093\n",
      "Epoch 45/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2192.3274 - mean_squared_error: 2192.3274\n",
      "Epoch 46/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2188.8311 - mean_squared_error: 2188.8311\n",
      "Epoch 47/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2185.3203 - mean_squared_error: 2185.3203\n",
      "Epoch 48/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2181.7983 - mean_squared_error: 2181.7983\n",
      "Epoch 49/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2178.2637 - mean_squared_error: 2178.2637\n",
      "Epoch 50/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2174.7185 - mean_squared_error: 2174.7185\n",
      "Epoch 51/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2171.1638 - mean_squared_error: 2171.1638\n",
      "Epoch 52/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2167.5986 - mean_squared_error: 2167.5986\n",
      "Epoch 53/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2164.0249 - mean_squared_error: 2164.0249\n",
      "Epoch 54/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2160.4424 - mean_squared_error: 2160.4424\n",
      "Epoch 55/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2156.8513 - mean_squared_error: 2156.8513\n",
      "Epoch 56/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2153.2520 - mean_squared_error: 2153.2520\n",
      "Epoch 57/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2149.6445 - mean_squared_error: 2149.6445\n",
      "Epoch 58/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2146.0295 - mean_squared_error: 2146.0295\n",
      "Epoch 59/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2142.4075 - mean_squared_error: 2142.4075\n",
      "Epoch 60/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2138.7783 - mean_squared_error: 2138.7783\n",
      "Epoch 61/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2135.1426 - mean_squared_error: 2135.1426\n",
      "Epoch 62/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2131.4993 - mean_squared_error: 2131.4993\n",
      "Epoch 63/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2127.8499 - mean_squared_error: 2127.8499\n",
      "Epoch 64/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2124.1948 - mean_squared_error: 2124.1948\n",
      "Epoch 65/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2120.5344 - mean_squared_error: 2120.5344\n",
      "Epoch 66/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2116.8684 - mean_squared_error: 2116.8684\n",
      "Epoch 67/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2113.1975 - mean_squared_error: 2113.1975\n",
      "Epoch 68/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2109.5222 - mean_squared_error: 2109.5222\n",
      "Epoch 69/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2105.8428 - mean_squared_error: 2105.8428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2102.1609 - mean_squared_error: 2102.1609\n",
      "Epoch 71/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2098.4751 - mean_squared_error: 2098.4751\n",
      "Epoch 72/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2094.7871 - mean_squared_error: 2094.7871\n",
      "Epoch 73/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2091.0979 - mean_squared_error: 2091.0979\n",
      "Epoch 74/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2087.4072 - mean_squared_error: 2087.4072\n",
      "Epoch 75/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2083.7156 - mean_squared_error: 2083.7156\n",
      "Epoch 76/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2080.0251 - mean_squared_error: 2080.0251\n",
      "Epoch 77/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2076.3342 - mean_squared_error: 2076.3342\n",
      "Epoch 78/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2072.6445 - mean_squared_error: 2072.6445\n",
      "Epoch 79/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2068.9578 - mean_squared_error: 2068.9578\n",
      "Epoch 80/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2065.2732 - mean_squared_error: 2065.2732\n",
      "Epoch 81/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2061.5925 - mean_squared_error: 2061.5925\n",
      "Epoch 82/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2057.9153 - mean_squared_error: 2057.9153\n",
      "Epoch 83/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2054.2419 - mean_squared_error: 2054.2419\n",
      "Epoch 84/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2050.5740 - mean_squared_error: 2050.5740\n",
      "Epoch 85/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2046.9117 - mean_squared_error: 2046.9117\n",
      "Epoch 86/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2043.2557 - mean_squared_error: 2043.2557\n",
      "Epoch 87/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2039.6069 - mean_squared_error: 2039.6069\n",
      "Epoch 88/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2035.9655 - mean_squared_error: 2035.9655\n",
      "Epoch 89/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2032.3307 - mean_squared_error: 2032.3307\n",
      "Epoch 90/400\n",
      "468/468 [==============================] - 0s 2us/sample - loss: 2028.7057 - mean_squared_error: 2028.7057\n",
      "Epoch 91/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2025.0885 - mean_squared_error: 2025.0885\n",
      "Epoch 92/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2021.4817 - mean_squared_error: 2021.4817\n",
      "Epoch 93/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2017.8837 - mean_squared_error: 2017.8837\n",
      "Epoch 94/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2014.2969 - mean_squared_error: 2014.2969\n",
      "Epoch 95/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2010.7198 - mean_squared_error: 2010.7198\n",
      "Epoch 96/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2007.1536 - mean_squared_error: 2007.1536\n",
      "Epoch 97/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2003.5994 - mean_squared_error: 2003.5994\n",
      "Epoch 98/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2000.0564 - mean_squared_error: 2000.0564\n",
      "Epoch 99/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1996.5259 - mean_squared_error: 1996.5259\n",
      "Epoch 100/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1993.0065 - mean_squared_error: 1993.0065\n",
      "Epoch 101/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1989.5012 - mean_squared_error: 1989.5012\n",
      "Epoch 102/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1986.0077 - mean_squared_error: 1986.0077\n",
      "Epoch 103/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1982.5273 - mean_squared_error: 1982.5273\n",
      "Epoch 104/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1979.0612 - mean_squared_error: 1979.0612\n",
      "Epoch 105/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1975.6079 - mean_squared_error: 1975.6079\n",
      "Epoch 106/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1972.1682 - mean_squared_error: 1972.1682\n",
      "Epoch 107/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1968.7424 - mean_squared_error: 1968.7424\n",
      "Epoch 108/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1965.3312 - mean_squared_error: 1965.3312\n",
      "Epoch 109/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1961.9342 - mean_squared_error: 1961.9342\n",
      "Epoch 110/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1958.5510 - mean_squared_error: 1958.5510\n",
      "Epoch 111/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1955.1823 - mean_squared_error: 1955.1823\n",
      "Epoch 112/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1951.8281 - mean_squared_error: 1951.8281\n",
      "Epoch 113/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1948.4888 - mean_squared_error: 1948.4888\n",
      "Epoch 114/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1945.1643 - mean_squared_error: 1945.1643\n",
      "Epoch 115/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1941.8540 - mean_squared_error: 1941.8540\n",
      "Epoch 116/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1938.5587 - mean_squared_error: 1938.5587\n",
      "Epoch 117/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1935.2781 - mean_squared_error: 1935.2781\n",
      "Epoch 118/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1932.0131 - mean_squared_error: 1932.0131\n",
      "Epoch 119/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1928.7625 - mean_squared_error: 1928.7625\n",
      "Epoch 120/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1925.5271 - mean_squared_error: 1925.5271\n",
      "Epoch 121/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1922.3059 - mean_squared_error: 1922.3059\n",
      "Epoch 122/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1919.1000 - mean_squared_error: 1919.1000\n",
      "Epoch 123/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1915.9088 - mean_squared_error: 1915.9088\n",
      "Epoch 124/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1912.7324 - mean_squared_error: 1912.7324\n",
      "Epoch 125/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1909.5702 - mean_squared_error: 1909.5702\n",
      "Epoch 126/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1906.4230 - mean_squared_error: 1906.4230\n",
      "Epoch 127/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1903.2909 - mean_squared_error: 1903.2909\n",
      "Epoch 128/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1900.1730 - mean_squared_error: 1900.1730\n",
      "Epoch 129/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1897.0692 - mean_squared_error: 1897.0692\n",
      "Epoch 130/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1893.9803 - mean_squared_error: 1893.9803\n",
      "Epoch 131/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1890.9050 - mean_squared_error: 1890.9050\n",
      "Epoch 132/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1887.8448 - mean_squared_error: 1887.8448\n",
      "Epoch 133/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1884.7979 - mean_squared_error: 1884.7979\n",
      "Epoch 134/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1881.7656 - mean_squared_error: 1881.7656\n",
      "Epoch 135/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1878.7471 - mean_squared_error: 1878.7471\n",
      "Epoch 136/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1875.7427 - mean_squared_error: 1875.7427\n",
      "Epoch 137/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1872.7513 - mean_squared_error: 1872.7513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1869.7738 - mean_squared_error: 1869.7738\n",
      "Epoch 139/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1866.8103 - mean_squared_error: 1866.8103\n",
      "Epoch 140/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1863.8595 - mean_squared_error: 1863.8595\n",
      "Epoch 141/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1860.9219 - mean_squared_error: 1860.9219\n",
      "Epoch 142/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1857.9980 - mean_squared_error: 1857.9980\n",
      "Epoch 143/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1855.0870 - mean_squared_error: 1855.0870\n",
      "Epoch 144/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1852.1891 - mean_squared_error: 1852.1891\n",
      "Epoch 145/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1849.3033 - mean_squared_error: 1849.3033\n",
      "Epoch 146/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1846.4305 - mean_squared_error: 1846.4305\n",
      "Epoch 147/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1843.5713 - mean_squared_error: 1843.5713\n",
      "Epoch 148/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1840.7230 - mean_squared_error: 1840.7230\n",
      "Epoch 149/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1837.8887 - mean_squared_error: 1837.8887\n",
      "Epoch 150/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1835.0659 - mean_squared_error: 1835.0659\n",
      "Epoch 151/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1832.2549 - mean_squared_error: 1832.2549\n",
      "Epoch 152/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1829.4575 - mean_squared_error: 1829.4575\n",
      "Epoch 153/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1826.6709 - mean_squared_error: 1826.6709\n",
      "Epoch 154/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1823.8965 - mean_squared_error: 1823.8965\n",
      "Epoch 155/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1821.1348 - mean_squared_error: 1821.1348\n",
      "Epoch 156/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1818.3840 - mean_squared_error: 1818.3840\n",
      "Epoch 157/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1815.6450 - mean_squared_error: 1815.6450\n",
      "Epoch 158/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1812.9177 - mean_squared_error: 1812.9177\n",
      "Epoch 159/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1810.2021 - mean_squared_error: 1810.2021\n",
      "Epoch 160/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1807.4976 - mean_squared_error: 1807.4976\n",
      "Epoch 161/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1804.8044 - mean_squared_error: 1804.8044\n",
      "Epoch 162/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1802.1233 - mean_squared_error: 1802.1233\n",
      "Epoch 163/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1799.4526 - mean_squared_error: 1799.4526\n",
      "Epoch 164/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1796.7941 - mean_squared_error: 1796.7941\n",
      "Epoch 165/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1794.1459 - mean_squared_error: 1794.1459\n",
      "Epoch 166/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1791.5087 - mean_squared_error: 1791.5087\n",
      "Epoch 167/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1788.8831 - mean_squared_error: 1788.8831\n",
      "Epoch 168/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1786.2682 - mean_squared_error: 1786.2682\n",
      "Epoch 169/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1783.6638 - mean_squared_error: 1783.6638\n",
      "Epoch 170/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1781.0706 - mean_squared_error: 1781.0706\n",
      "Epoch 171/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1778.4886 - mean_squared_error: 1778.4886\n",
      "Epoch 172/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1775.9171 - mean_squared_error: 1775.9171\n",
      "Epoch 173/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1773.3553 - mean_squared_error: 1773.3553\n",
      "Epoch 174/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1770.8057 - mean_squared_error: 1770.8057\n",
      "Epoch 175/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1768.2662 - mean_squared_error: 1768.2662\n",
      "Epoch 176/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1765.7372 - mean_squared_error: 1765.7372\n",
      "Epoch 177/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1763.2189 - mean_squared_error: 1763.2189\n",
      "Epoch 178/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1760.7106 - mean_squared_error: 1760.7106\n",
      "Epoch 179/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1758.2130 - mean_squared_error: 1758.2130\n",
      "Epoch 180/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1755.7263 - mean_squared_error: 1755.7263\n",
      "Epoch 181/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1753.2496 - mean_squared_error: 1753.2496\n",
      "Epoch 182/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1750.7832 - mean_squared_error: 1750.7832\n",
      "Epoch 183/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1748.3271 - mean_squared_error: 1748.3271\n",
      "Epoch 184/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1745.8812 - mean_squared_error: 1745.8812\n",
      "Epoch 185/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1743.4462 - mean_squared_error: 1743.4462\n",
      "Epoch 186/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1741.0214 - mean_squared_error: 1741.0214\n",
      "Epoch 187/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1738.6057 - mean_squared_error: 1738.6057\n",
      "Epoch 188/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1736.2014 - mean_squared_error: 1736.2014\n",
      "Epoch 189/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1733.8058 - mean_squared_error: 1733.8058\n",
      "Epoch 190/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1731.4219 - mean_squared_error: 1731.4219\n",
      "Epoch 191/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1729.0466 - mean_squared_error: 1729.0466\n",
      "Epoch 192/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1726.6823 - mean_squared_error: 1726.6823\n",
      "Epoch 193/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1724.3280 - mean_squared_error: 1724.3280\n",
      "Epoch 194/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1721.9829 - mean_squared_error: 1721.9829\n",
      "Epoch 195/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1719.6481 - mean_squared_error: 1719.6481\n",
      "Epoch 196/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1717.3236 - mean_squared_error: 1717.3236\n",
      "Epoch 197/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1715.0085 - mean_squared_error: 1715.0085\n",
      "Epoch 198/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1712.7040 - mean_squared_error: 1712.7040\n",
      "Epoch 199/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1710.4087 - mean_squared_error: 1710.4087\n",
      "Epoch 200/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1708.1230 - mean_squared_error: 1708.1230\n",
      "Epoch 201/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1705.8474 - mean_squared_error: 1705.8474\n",
      "Epoch 202/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1703.5814 - mean_squared_error: 1703.5814\n",
      "Epoch 203/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1701.3250 - mean_squared_error: 1701.3250\n",
      "Epoch 204/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1699.0781 - mean_squared_error: 1699.0781\n",
      "Epoch 205/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1696.8413 - mean_squared_error: 1696.8413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1694.6135 - mean_squared_error: 1694.6135\n",
      "Epoch 207/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1692.3959 - mean_squared_error: 1692.3959\n",
      "Epoch 208/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1690.1874 - mean_squared_error: 1690.1874\n",
      "Epoch 209/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1687.9880 - mean_squared_error: 1687.9880\n",
      "Epoch 210/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1685.7981 - mean_squared_error: 1685.7981\n",
      "Epoch 211/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1683.6184 - mean_squared_error: 1683.6184\n",
      "Epoch 212/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1681.4475 - mean_squared_error: 1681.4475\n",
      "Epoch 213/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1679.2859 - mean_squared_error: 1679.2859\n",
      "Epoch 214/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1677.1344 - mean_squared_error: 1677.1344\n",
      "Epoch 215/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1674.9915 - mean_squared_error: 1674.9915\n",
      "Epoch 216/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1672.8577 - mean_squared_error: 1672.8577\n",
      "Epoch 217/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1670.7330 - mean_squared_error: 1670.7330\n",
      "Epoch 218/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1668.6177 - mean_squared_error: 1668.6177\n",
      "Epoch 219/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1666.5115 - mean_squared_error: 1666.5115\n",
      "Epoch 220/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1664.4142 - mean_squared_error: 1664.4142\n",
      "Epoch 221/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1662.3265 - mean_squared_error: 1662.3265\n",
      "Epoch 222/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1660.2474 - mean_squared_error: 1660.2474\n",
      "Epoch 223/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1658.1774 - mean_squared_error: 1658.1774\n",
      "Epoch 224/400\n",
      "468/468 [==============================] - 0s 2us/sample - loss: 1656.1162 - mean_squared_error: 1656.1162\n",
      "Epoch 225/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1654.0641 - mean_squared_error: 1654.0641\n",
      "Epoch 226/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1652.0204 - mean_squared_error: 1652.0204\n",
      "Epoch 227/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1649.9857 - mean_squared_error: 1649.9857\n",
      "Epoch 228/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1647.9601 - mean_squared_error: 1647.9601\n",
      "Epoch 229/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1645.9431 - mean_squared_error: 1645.9431\n",
      "Epoch 230/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1643.9347 - mean_squared_error: 1643.9347\n",
      "Epoch 231/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1641.9349 - mean_squared_error: 1641.9349\n",
      "Epoch 232/400\n",
      "468/468 [==============================] - 0s 7us/sample - loss: 1639.9441 - mean_squared_error: 1639.9441\n",
      "Epoch 233/400\n",
      "468/468 [==============================] - 0s 5us/sample - loss: 1637.9615 - mean_squared_error: 1637.9615\n",
      "Epoch 234/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1635.9880 - mean_squared_error: 1635.9880\n",
      "Epoch 235/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1634.0222 - mean_squared_error: 1634.0222\n",
      "Epoch 236/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1632.0653 - mean_squared_error: 1632.0653\n",
      "Epoch 237/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1630.1165 - mean_squared_error: 1630.1165\n",
      "Epoch 238/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1628.1761 - mean_squared_error: 1628.1761\n",
      "Epoch 239/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1626.2445 - mean_squared_error: 1626.2445\n",
      "Epoch 240/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1624.3207 - mean_squared_error: 1624.3207\n",
      "Epoch 241/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1622.4060 - mean_squared_error: 1622.4060\n",
      "Epoch 242/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1620.4989 - mean_squared_error: 1620.4989\n",
      "Epoch 243/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1618.6002 - mean_squared_error: 1618.6002\n",
      "Epoch 244/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1616.7089 - mean_squared_error: 1616.7089\n",
      "Epoch 245/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1614.8267 - mean_squared_error: 1614.8267\n",
      "Epoch 246/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1612.9517 - mean_squared_error: 1612.9517\n",
      "Epoch 247/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1611.0848 - mean_squared_error: 1611.0848\n",
      "Epoch 248/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1609.2263 - mean_squared_error: 1609.2263\n",
      "Epoch 249/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1607.3761 - mean_squared_error: 1607.3761\n",
      "Epoch 250/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1605.5331 - mean_squared_error: 1605.5331\n",
      "Epoch 251/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1603.6976 - mean_squared_error: 1603.6976\n",
      "Epoch 252/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1601.8708 - mean_squared_error: 1601.8708\n",
      "Epoch 253/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1600.0515 - mean_squared_error: 1600.0515\n",
      "Epoch 254/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1598.2395 - mean_squared_error: 1598.2395\n",
      "Epoch 255/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1596.4360 - mean_squared_error: 1596.4360\n",
      "Epoch 256/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1594.6392 - mean_squared_error: 1594.6392\n",
      "Epoch 257/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1592.8508 - mean_squared_error: 1592.8508\n",
      "Epoch 258/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1591.0692 - mean_squared_error: 1591.0692\n",
      "Epoch 259/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1589.2954 - mean_squared_error: 1589.2954\n",
      "Epoch 260/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1587.5289 - mean_squared_error: 1587.5289\n",
      "Epoch 261/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1585.7698 - mean_squared_error: 1585.7698\n",
      "Epoch 262/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1584.0183 - mean_squared_error: 1584.0183\n",
      "Epoch 263/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1582.2739 - mean_squared_error: 1582.2739\n",
      "Epoch 264/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1580.5375 - mean_squared_error: 1580.5375\n",
      "Epoch 265/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1578.8073 - mean_squared_error: 1578.8073\n",
      "Epoch 266/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1577.0850 - mean_squared_error: 1577.0850\n",
      "Epoch 267/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1575.3695 - mean_squared_error: 1575.3695\n",
      "Epoch 268/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1573.6611 - mean_squared_error: 1573.6611\n",
      "Epoch 269/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1571.9601 - mean_squared_error: 1571.9601\n",
      "Epoch 270/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1570.2656 - mean_squared_error: 1570.2656\n",
      "Epoch 271/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1568.5782 - mean_squared_error: 1568.5782\n",
      "Epoch 272/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1566.8975 - mean_squared_error: 1566.8975\n",
      "Epoch 273/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1565.2239 - mean_squared_error: 1565.2239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1563.5581 - mean_squared_error: 1563.5581\n",
      "Epoch 275/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1561.8982 - mean_squared_error: 1561.8982\n",
      "Epoch 276/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1560.2451 - mean_squared_error: 1560.2451\n",
      "Epoch 277/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1558.5983 - mean_squared_error: 1558.5983\n",
      "Epoch 278/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1556.9589 - mean_squared_error: 1556.9589\n",
      "Epoch 279/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1555.3254 - mean_squared_error: 1555.3254\n",
      "Epoch 280/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1553.6989 - mean_squared_error: 1553.6989\n",
      "Epoch 281/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1552.0782 - mean_squared_error: 1552.0782\n",
      "Epoch 282/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1550.4653 - mean_squared_error: 1550.4653\n",
      "Epoch 283/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1548.8579 - mean_squared_error: 1548.8579\n",
      "Epoch 284/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1547.2571 - mean_squared_error: 1547.2571\n",
      "Epoch 285/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1545.6625 - mean_squared_error: 1545.6625\n",
      "Epoch 286/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1544.0745 - mean_squared_error: 1544.0745\n",
      "Epoch 287/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1542.4923 - mean_squared_error: 1542.4923\n",
      "Epoch 288/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1540.9165 - mean_squared_error: 1540.9165\n",
      "Epoch 289/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1539.3472 - mean_squared_error: 1539.3472\n",
      "Epoch 290/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1537.7836 - mean_squared_error: 1537.7836\n",
      "Epoch 291/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1536.2258 - mean_squared_error: 1536.2258\n",
      "Epoch 292/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1534.6746 - mean_squared_error: 1534.6746\n",
      "Epoch 293/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1533.1290 - mean_squared_error: 1533.1290\n",
      "Epoch 294/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1531.5895 - mean_squared_error: 1531.5895\n",
      "Epoch 295/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1530.0558 - mean_squared_error: 1530.0558\n",
      "Epoch 296/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1528.5280 - mean_squared_error: 1528.5280\n",
      "Epoch 297/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1527.0060 - mean_squared_error: 1527.0060\n",
      "Epoch 298/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1525.4893 - mean_squared_error: 1525.4893\n",
      "Epoch 299/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1523.9783 - mean_squared_error: 1523.9783\n",
      "Epoch 300/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1522.4731 - mean_squared_error: 1522.4731\n",
      "Epoch 301/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1520.9740 - mean_squared_error: 1520.9740\n",
      "Epoch 302/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1519.4799 - mean_squared_error: 1519.4799\n",
      "Epoch 303/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1517.9916 - mean_squared_error: 1517.9916\n",
      "Epoch 304/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1516.5084 - mean_squared_error: 1516.5084\n",
      "Epoch 305/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1515.0309 - mean_squared_error: 1515.0309\n",
      "Epoch 306/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1513.5588 - mean_squared_error: 1513.5588\n",
      "Epoch 307/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1512.0916 - mean_squared_error: 1512.0916\n",
      "Epoch 308/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1510.6295 - mean_squared_error: 1510.6295\n",
      "Epoch 309/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1509.1733 - mean_squared_error: 1509.1733\n",
      "Epoch 310/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1507.7219 - mean_squared_error: 1507.7219\n",
      "Epoch 311/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1506.2748 - mean_squared_error: 1506.2748\n",
      "Epoch 312/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1504.8335 - mean_squared_error: 1504.8335\n",
      "Epoch 313/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1503.3978 - mean_squared_error: 1503.3978\n",
      "Epoch 314/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1501.9663 - mean_squared_error: 1501.9663\n",
      "Epoch 315/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1500.5400 - mean_squared_error: 1500.5400\n",
      "Epoch 316/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1499.1179 - mean_squared_error: 1499.1179\n",
      "Epoch 317/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1497.7014 - mean_squared_error: 1497.7014\n",
      "Epoch 318/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1496.2899 - mean_squared_error: 1496.2899\n",
      "Epoch 319/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1494.8821 - mean_squared_error: 1494.8821\n",
      "Epoch 320/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1493.4795 - mean_squared_error: 1493.4795\n",
      "Epoch 321/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1492.0815 - mean_squared_error: 1492.0815\n",
      "Epoch 322/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1490.6876 - mean_squared_error: 1490.6876\n",
      "Epoch 323/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1489.2992 - mean_squared_error: 1489.2992\n",
      "Epoch 324/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1487.9138 - mean_squared_error: 1487.9138\n",
      "Epoch 325/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1486.5342 - mean_squared_error: 1486.5342\n",
      "Epoch 326/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1485.1584 - mean_squared_error: 1485.1584\n",
      "Epoch 327/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1483.7866 - mean_squared_error: 1483.7866\n",
      "Epoch 328/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1482.4193 - mean_squared_error: 1482.4193\n",
      "Epoch 329/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1481.0560 - mean_squared_error: 1481.0560\n",
      "Epoch 330/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1479.6970 - mean_squared_error: 1479.6970\n",
      "Epoch 331/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1478.3423 - mean_squared_error: 1478.3423\n",
      "Epoch 332/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1476.9913 - mean_squared_error: 1476.9913\n",
      "Epoch 333/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1475.6448 - mean_squared_error: 1475.6448\n",
      "Epoch 334/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1474.3016 - mean_squared_error: 1474.3016\n",
      "Epoch 335/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1472.9623 - mean_squared_error: 1472.9623\n",
      "Epoch 336/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1471.6270 - mean_squared_error: 1471.6270\n",
      "Epoch 337/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1470.2957 - mean_squared_error: 1470.2957\n",
      "Epoch 338/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1468.9675 - mean_squared_error: 1468.9675\n",
      "Epoch 339/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1467.6440 - mean_squared_error: 1467.6440\n",
      "Epoch 340/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1466.3234 - mean_squared_error: 1466.3234\n",
      "Epoch 341/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1465.0060 - mean_squared_error: 1465.0060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1463.6927 - mean_squared_error: 1463.6927\n",
      "Epoch 343/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1462.3823 - mean_squared_error: 1462.3823\n",
      "Epoch 344/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1461.0751 - mean_squared_error: 1461.0751\n",
      "Epoch 345/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1459.7723 - mean_squared_error: 1459.7723\n",
      "Epoch 346/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1458.4719 - mean_squared_error: 1458.4719\n",
      "Epoch 347/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1457.1749 - mean_squared_error: 1457.1749\n",
      "Epoch 348/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1455.8807 - mean_squared_error: 1455.8807\n",
      "Epoch 349/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1454.5906 - mean_squared_error: 1454.5906\n",
      "Epoch 350/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1453.3027 - mean_squared_error: 1453.3027\n",
      "Epoch 351/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1452.0181 - mean_squared_error: 1452.0181\n",
      "Epoch 352/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1450.7358 - mean_squared_error: 1450.7358\n",
      "Epoch 353/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1449.4570 - mean_squared_error: 1449.4570\n",
      "Epoch 354/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1448.1807 - mean_squared_error: 1448.1807\n",
      "Epoch 355/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1446.9066 - mean_squared_error: 1446.9066\n",
      "Epoch 356/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1445.6360 - mean_squared_error: 1445.6360\n",
      "Epoch 357/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1444.3671 - mean_squared_error: 1444.3671\n",
      "Epoch 358/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1443.1011 - mean_squared_error: 1443.1011\n",
      "Epoch 359/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1441.8373 - mean_squared_error: 1441.8373\n",
      "Epoch 360/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1440.5758 - mean_squared_error: 1440.5758\n",
      "Epoch 361/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1439.3162 - mean_squared_error: 1439.3162\n",
      "Epoch 362/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1438.0598 - mean_squared_error: 1438.0598\n",
      "Epoch 363/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1436.8044 - mean_squared_error: 1436.8044\n",
      "Epoch 364/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1435.5515 - mean_squared_error: 1435.5515\n",
      "Epoch 365/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1434.3003 - mean_squared_error: 1434.3003\n",
      "Epoch 366/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 1433.0505 - mean_squared_error: 1433.0505\n",
      "Epoch 367/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1431.8024 - mean_squared_error: 1431.8024\n",
      "Epoch 368/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1430.5559 - mean_squared_error: 1430.5559\n",
      "Epoch 369/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1429.3115 - mean_squared_error: 1429.3115\n",
      "Epoch 370/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1428.0670 - mean_squared_error: 1428.0670\n",
      "Epoch 371/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1426.8240 - mean_squared_error: 1426.8240\n",
      "Epoch 372/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1425.5828 - mean_squared_error: 1425.5828\n",
      "Epoch 373/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1424.3419 - mean_squared_error: 1424.3419\n",
      "Epoch 374/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1423.1018 - mean_squared_error: 1423.1018\n",
      "Epoch 375/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1421.8624 - mean_squared_error: 1421.8624\n",
      "Epoch 376/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1420.6227 - mean_squared_error: 1420.6227\n",
      "Epoch 377/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1419.3845 - mean_squared_error: 1419.3845\n",
      "Epoch 378/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1418.1451 - mean_squared_error: 1418.1451\n",
      "Epoch 379/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1416.9065 - mean_squared_error: 1416.9065\n",
      "Epoch 380/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1415.6674 - mean_squared_error: 1415.6674\n",
      "Epoch 381/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1414.4277 - mean_squared_error: 1414.4277\n",
      "Epoch 382/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1413.1869 - mean_squared_error: 1413.1869\n",
      "Epoch 383/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1411.9459 - mean_squared_error: 1411.9459\n",
      "Epoch 384/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1410.7029 - mean_squared_error: 1410.7029\n",
      "Epoch 385/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1409.4590 - mean_squared_error: 1409.4590\n",
      "Epoch 386/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1408.2136 - mean_squared_error: 1408.2136\n",
      "Epoch 387/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1406.9662 - mean_squared_error: 1406.9662\n",
      "Epoch 388/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1405.7172 - mean_squared_error: 1405.7172\n",
      "Epoch 389/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1404.4655 - mean_squared_error: 1404.4655\n",
      "Epoch 390/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1403.2115 - mean_squared_error: 1403.2115\n",
      "Epoch 391/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1401.9551 - mean_squared_error: 1401.9551\n",
      "Epoch 392/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1400.6967 - mean_squared_error: 1400.6967\n",
      "Epoch 393/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1399.4341 - mean_squared_error: 1399.4341\n",
      "Epoch 394/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1398.1697 - mean_squared_error: 1398.1697\n",
      "Epoch 395/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1396.9020 - mean_squared_error: 1396.9020\n",
      "Epoch 396/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1395.6312 - mean_squared_error: 1395.6312\n",
      "Epoch 397/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1394.3582 - mean_squared_error: 1394.3582\n",
      "Epoch 398/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 1393.0822 - mean_squared_error: 1393.0822\n",
      "Epoch 399/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1391.8030 - mean_squared_error: 1391.8030\n",
      "Epoch 400/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 1390.5217 - mean_squared_error: 1390.5217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b45ec66828>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#step 1: build the model\n",
    "model1 = Sequential()\n",
    "#input layer\n",
    "model1.add(Dense(10, input_dim = 31, activation = 'sigmoid'))\n",
    "#output layer: - no activation function is needed for regression\n",
    "model1.add(Dense(1))\n",
    "#step 2: compile the model\n",
    "model1.compile(loss = 'mse', optimizer = 'sgd', metrics = ['mse'])\n",
    "#step 3: train the model\n",
    "model1.fit(X_train,y_train, epochs = 400, batch_size = 468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 0s 170us/sample - loss: 1389.2383 - mean_squared_error: 1389.2383\n",
      "157/157 [==============================] - 0s 44us/sample - loss: 43.5710 - mean_squared_error: 43.5710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[43.57096940089183, 43.57097]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_train, y_train)\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred training score for neutral network: 0.3983534887482504\n",
      "R_squred testing score for neutral network: 0.9017681120520391\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model1.predict(X_train)\n",
    "y_pred_test = model1.predict(X_test)\n",
    "print('R_squred training score for neutral network: {}'.format(r2_score(y_train, y_pred_train)))\n",
    "print('R_squred testing score for neutral network: {}'.format(r2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning: Neural Networks with Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "468/468 [==============================] - 0s 299us/sample - loss: 2400.6396 - mean_squared_error: 2400.6396\n",
      "Epoch 2/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2389.9214 - mean_squared_error: 2389.9214\n",
      "Epoch 3/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2385.0105 - mean_squared_error: 2385.0105\n",
      "Epoch 4/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2379.0669 - mean_squared_error: 2379.0669\n",
      "Epoch 5/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2370.6489 - mean_squared_error: 2370.6489\n",
      "Epoch 6/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2357.0120 - mean_squared_error: 2357.0120\n",
      "Epoch 7/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2333.9690 - mean_squared_error: 2333.9690\n",
      "Epoch 8/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2305.6394 - mean_squared_error: 2305.6394\n",
      "Epoch 9/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2294.7629 - mean_squared_error: 2294.7629\n",
      "Epoch 10/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2286.1558 - mean_squared_error: 2286.1558\n",
      "Epoch 11/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2276.1677 - mean_squared_error: 2276.1677\n",
      "Epoch 12/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2265.8682 - mean_squared_error: 2265.8682\n",
      "Epoch 13/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2254.1145 - mean_squared_error: 2254.1145\n",
      "Epoch 14/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2254.9453 - mean_squared_error: 2254.9453\n",
      "Epoch 15/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2251.5835 - mean_squared_error: 2251.5835\n",
      "Epoch 16/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2375.9458 - mean_squared_error: 2375.9458\n",
      "Epoch 17/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2357.3577 - mean_squared_error: 2357.3577\n",
      "Epoch 18/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2319.3730 - mean_squared_error: 2319.3730\n",
      "Epoch 19/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2232.9910 - mean_squared_error: 2232.9910\n",
      "Epoch 20/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2203.7473 - mean_squared_error: 2203.7473\n",
      "Epoch 21/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2326.2195 - mean_squared_error: 2326.2195\n",
      "Epoch 22/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2142.6541 - mean_squared_error: 2142.6541\n",
      "Epoch 23/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2306.7058 - mean_squared_error: 2306.7058\n",
      "Epoch 24/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2376.9270 - mean_squared_error: 2376.9270\n",
      "Epoch 25/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2374.2397 - mean_squared_error: 2374.2397\n",
      "Epoch 26/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2371.6587 - mean_squared_error: 2371.6587\n",
      "Epoch 27/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2369.1797 - mean_squared_error: 2369.1797\n",
      "Epoch 28/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2366.7983 - mean_squared_error: 2366.7983\n",
      "Epoch 29/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2364.5120 - mean_squared_error: 2364.5120\n",
      "Epoch 30/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2362.3169 - mean_squared_error: 2362.3169\n",
      "Epoch 31/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2360.2078 - mean_squared_error: 2360.2078\n",
      "Epoch 32/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2358.1819 - mean_squared_error: 2358.1819\n",
      "Epoch 33/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2356.2368 - mean_squared_error: 2356.2368\n",
      "Epoch 34/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2354.3687 - mean_squared_error: 2354.3687\n",
      "Epoch 35/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2352.5742 - mean_squared_error: 2352.5742\n",
      "Epoch 36/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2350.8513 - mean_squared_error: 2350.8513\n",
      "Epoch 37/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2349.1965 - mean_squared_error: 2349.1965\n",
      "Epoch 38/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2347.6069 - mean_squared_error: 2347.6069\n",
      "Epoch 39/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2346.0806 - mean_squared_error: 2346.0806\n",
      "Epoch 40/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2344.6145 - mean_squared_error: 2344.6145\n",
      "Epoch 41/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2343.2065 - mean_squared_error: 2343.2065\n",
      "Epoch 42/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2341.8545 - mean_squared_error: 2341.8545\n",
      "Epoch 43/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2340.5552 - mean_squared_error: 2340.5552\n",
      "Epoch 44/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2339.3086 - mean_squared_error: 2339.3086\n",
      "Epoch 45/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2338.1106 - mean_squared_error: 2338.1106\n",
      "Epoch 46/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2336.9595 - mean_squared_error: 2336.9595\n",
      "Epoch 47/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2335.8555 - mean_squared_error: 2335.8555\n",
      "Epoch 48/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2334.7942 - mean_squared_error: 2334.7942\n",
      "Epoch 49/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2333.7756 - mean_squared_error: 2333.7756\n",
      "Epoch 50/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2332.7964 - mean_squared_error: 2332.7964\n",
      "Epoch 51/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2331.8567 - mean_squared_error: 2331.8567\n",
      "Epoch 52/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2330.9536 - mean_squared_error: 2330.9536\n",
      "Epoch 53/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2330.0869 - mean_squared_error: 2330.0869\n",
      "Epoch 54/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2329.2539 - mean_squared_error: 2329.2539\n",
      "Epoch 55/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2328.4546 - mean_squared_error: 2328.4546\n",
      "Epoch 56/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2327.6868 - mean_squared_error: 2327.6868\n",
      "Epoch 57/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2326.9490 - mean_squared_error: 2326.9490\n",
      "Epoch 58/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2326.2410 - mean_squared_error: 2326.2410\n",
      "Epoch 59/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2325.5601 - mean_squared_error: 2325.5601\n",
      "Epoch 60/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2324.9070 - mean_squared_error: 2324.9070\n",
      "Epoch 61/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2324.2795 - mean_squared_error: 2324.2795\n",
      "Epoch 62/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2323.6768 - mean_squared_error: 2323.6768\n",
      "Epoch 63/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2323.0981 - mean_squared_error: 2323.0981\n",
      "Epoch 64/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2322.5422 - mean_squared_error: 2322.5422\n",
      "Epoch 65/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2322.0081 - mean_squared_error: 2322.0081\n",
      "Epoch 66/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2321.4961 - mean_squared_error: 2321.4961\n",
      "Epoch 67/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2321.0024 - mean_squared_error: 2321.0024\n",
      "Epoch 68/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2320.5303 - mean_squared_error: 2320.5303\n",
      "Epoch 69/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2320.0759 - mean_squared_error: 2320.0759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2319.6399 - mean_squared_error: 2319.6399\n",
      "Epoch 71/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2319.2207 - mean_squared_error: 2319.2207\n",
      "Epoch 72/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2318.8179 - mean_squared_error: 2318.8179\n",
      "Epoch 73/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2318.4319 - mean_squared_error: 2318.4319\n",
      "Epoch 74/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2318.0605 - mean_squared_error: 2318.0605\n",
      "Epoch 75/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2317.7041 - mean_squared_error: 2317.7041\n",
      "Epoch 76/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2317.3625 - mean_squared_error: 2317.3625\n",
      "Epoch 77/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2317.0332 - mean_squared_error: 2317.0332\n",
      "Epoch 78/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2316.7175 - mean_squared_error: 2316.7175\n",
      "Epoch 79/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2316.4141 - mean_squared_error: 2316.4141\n",
      "Epoch 80/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2316.1228 - mean_squared_error: 2316.1228\n",
      "Epoch 81/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2315.8430 - mean_squared_error: 2315.8430\n",
      "Epoch 82/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2315.5747 - mean_squared_error: 2315.5747\n",
      "Epoch 83/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2315.3169 - mean_squared_error: 2315.3169\n",
      "Epoch 84/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2315.0686 - mean_squared_error: 2315.0686\n",
      "Epoch 85/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2314.8318 - mean_squared_error: 2314.8318\n",
      "Epoch 86/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2314.6025 - mean_squared_error: 2314.6025\n",
      "Epoch 87/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2314.3828 - mean_squared_error: 2314.3828\n",
      "Epoch 88/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2314.1726 - mean_squared_error: 2314.1726\n",
      "Epoch 89/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.9700 - mean_squared_error: 2313.9700\n",
      "Epoch 90/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.7754 - mean_squared_error: 2313.7754\n",
      "Epoch 91/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.5889 - mean_squared_error: 2313.5889\n",
      "Epoch 92/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.4099 - mean_squared_error: 2313.4099\n",
      "Epoch 93/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.2371 - mean_squared_error: 2313.2371\n",
      "Epoch 94/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2313.0715 - mean_squared_error: 2313.0715\n",
      "Epoch 95/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.9128 - mean_squared_error: 2312.9128\n",
      "Epoch 96/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.7605 - mean_squared_error: 2312.7605\n",
      "Epoch 97/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2312.6143 - mean_squared_error: 2312.6143\n",
      "Epoch 98/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.4731 - mean_squared_error: 2312.4731\n",
      "Epoch 99/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.3384 - mean_squared_error: 2312.3384\n",
      "Epoch 100/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.2078 - mean_squared_error: 2312.2078\n",
      "Epoch 101/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2312.0833 - mean_squared_error: 2312.0833\n",
      "Epoch 102/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.9639 - mean_squared_error: 2311.9639\n",
      "Epoch 103/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.8486 - mean_squared_error: 2311.8486\n",
      "Epoch 104/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.7380 - mean_squared_error: 2311.7380\n",
      "Epoch 105/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.6316 - mean_squared_error: 2311.6316\n",
      "Epoch 106/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2311.5295 - mean_squared_error: 2311.5295\n",
      "Epoch 107/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2311.4331 - mean_squared_error: 2311.4331\n",
      "Epoch 108/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.3389 - mean_squared_error: 2311.3389\n",
      "Epoch 109/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2311.2488 - mean_squared_error: 2311.2488\n",
      "Epoch 110/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2311.1616 - mean_squared_error: 2311.1616\n",
      "Epoch 111/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2311.0789 - mean_squared_error: 2311.0789\n",
      "Epoch 112/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.9988 - mean_squared_error: 2310.9988\n",
      "Epoch 113/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.9221 - mean_squared_error: 2310.9221\n",
      "Epoch 114/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.8491 - mean_squared_error: 2310.8491\n",
      "Epoch 115/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.7778 - mean_squared_error: 2310.7778\n",
      "Epoch 116/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.7097 - mean_squared_error: 2310.7097\n",
      "Epoch 117/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.6443 - mean_squared_error: 2310.6443\n",
      "Epoch 118/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.5815 - mean_squared_error: 2310.5815\n",
      "Epoch 119/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.5210 - mean_squared_error: 2310.5210\n",
      "Epoch 120/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.4631 - mean_squared_error: 2310.4631\n",
      "Epoch 121/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.4080 - mean_squared_error: 2310.4080\n",
      "Epoch 122/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.3545 - mean_squared_error: 2310.3545\n",
      "Epoch 123/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.3040 - mean_squared_error: 2310.3040\n",
      "Epoch 124/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.2537 - mean_squared_error: 2310.2537\n",
      "Epoch 125/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.2068 - mean_squared_error: 2310.2068\n",
      "Epoch 126/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.1616 - mean_squared_error: 2310.1616\n",
      "Epoch 127/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.1182 - mean_squared_error: 2310.1182\n",
      "Epoch 128/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2310.0759 - mean_squared_error: 2310.0759\n",
      "Epoch 129/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2310.0361 - mean_squared_error: 2310.0361\n",
      "Epoch 130/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.9968 - mean_squared_error: 2309.9968\n",
      "Epoch 131/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.9600 - mean_squared_error: 2309.9600\n",
      "Epoch 132/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.9243 - mean_squared_error: 2309.9243\n",
      "Epoch 133/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.8901 - mean_squared_error: 2309.8901\n",
      "Epoch 134/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.8572 - mean_squared_error: 2309.8572\n",
      "Epoch 135/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.8257 - mean_squared_error: 2309.8257\n",
      "Epoch 136/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.7954 - mean_squared_error: 2309.7954\n",
      "Epoch 137/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.7664 - mean_squared_error: 2309.7664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.7380 - mean_squared_error: 2309.7380\n",
      "Epoch 139/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.7112 - mean_squared_error: 2309.7112\n",
      "Epoch 140/400\n",
      "468/468 [==============================] - 0s 5us/sample - loss: 2309.6858 - mean_squared_error: 2309.6858\n",
      "Epoch 141/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.6609 - mean_squared_error: 2309.6609\n",
      "Epoch 142/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.6367 - mean_squared_error: 2309.6367\n",
      "Epoch 143/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.6140 - mean_squared_error: 2309.6140\n",
      "Epoch 144/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.5925 - mean_squared_error: 2309.5925\n",
      "Epoch 145/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.5715 - mean_squared_error: 2309.5715\n",
      "Epoch 146/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.5515 - mean_squared_error: 2309.5515\n",
      "Epoch 147/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.5315 - mean_squared_error: 2309.5315\n",
      "Epoch 148/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.5132 - mean_squared_error: 2309.5132\n",
      "Epoch 149/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.4949 - mean_squared_error: 2309.4949\n",
      "Epoch 150/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.4773 - mean_squared_error: 2309.4773\n",
      "Epoch 151/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.4612 - mean_squared_error: 2309.4612\n",
      "Epoch 152/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.4453 - mean_squared_error: 2309.4453\n",
      "Epoch 153/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.4302 - mean_squared_error: 2309.4302\n",
      "Epoch 154/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.4153 - mean_squared_error: 2309.4153\n",
      "Epoch 155/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.4023 - mean_squared_error: 2309.4023\n",
      "Epoch 156/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3879 - mean_squared_error: 2309.3879\n",
      "Epoch 157/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3752 - mean_squared_error: 2309.3752\n",
      "Epoch 158/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3621 - mean_squared_error: 2309.3621\n",
      "Epoch 159/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3503 - mean_squared_error: 2309.3503\n",
      "Epoch 160/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3389 - mean_squared_error: 2309.3389\n",
      "Epoch 161/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3279 - mean_squared_error: 2309.3279\n",
      "Epoch 162/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.3174 - mean_squared_error: 2309.3174\n",
      "Epoch 163/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.3069 - mean_squared_error: 2309.3069\n",
      "Epoch 164/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2974 - mean_squared_error: 2309.2974\n",
      "Epoch 165/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2874 - mean_squared_error: 2309.2874\n",
      "Epoch 166/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2788 - mean_squared_error: 2309.2788\n",
      "Epoch 167/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2708 - mean_squared_error: 2309.2708\n",
      "Epoch 168/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2612 - mean_squared_error: 2309.2612\n",
      "Epoch 169/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2549 - mean_squared_error: 2309.2549\n",
      "Epoch 170/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2466 - mean_squared_error: 2309.2466\n",
      "Epoch 171/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.2388 - mean_squared_error: 2309.2388\n",
      "Epoch 172/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.2319 - mean_squared_error: 2309.2319\n",
      "Epoch 173/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2251 - mean_squared_error: 2309.2251\n",
      "Epoch 174/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.2188 - mean_squared_error: 2309.2188\n",
      "Epoch 175/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.2126 - mean_squared_error: 2309.2126\n",
      "Epoch 176/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2065 - mean_squared_error: 2309.2065\n",
      "Epoch 177/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.2007 - mean_squared_error: 2309.2007\n",
      "Epoch 178/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1951 - mean_squared_error: 2309.1951\n",
      "Epoch 179/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1897 - mean_squared_error: 2309.1897\n",
      "Epoch 180/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1843 - mean_squared_error: 2309.1843\n",
      "Epoch 181/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1797 - mean_squared_error: 2309.1797\n",
      "Epoch 182/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1746 - mean_squared_error: 2309.1746\n",
      "Epoch 183/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1702 - mean_squared_error: 2309.1702\n",
      "Epoch 184/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1655 - mean_squared_error: 2309.1655\n",
      "Epoch 185/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1616 - mean_squared_error: 2309.1616\n",
      "Epoch 186/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1582 - mean_squared_error: 2309.1582\n",
      "Epoch 187/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.1538 - mean_squared_error: 2309.1538\n",
      "Epoch 188/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1504 - mean_squared_error: 2309.1504\n",
      "Epoch 189/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1467 - mean_squared_error: 2309.1467\n",
      "Epoch 190/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1431 - mean_squared_error: 2309.1431\n",
      "Epoch 191/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1396 - mean_squared_error: 2309.1396\n",
      "Epoch 192/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1372 - mean_squared_error: 2309.1372\n",
      "Epoch 193/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1333 - mean_squared_error: 2309.1333\n",
      "Epoch 194/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1311 - mean_squared_error: 2309.1311\n",
      "Epoch 195/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1282 - mean_squared_error: 2309.1282\n",
      "Epoch 196/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1260 - mean_squared_error: 2309.1260\n",
      "Epoch 197/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1226 - mean_squared_error: 2309.1226\n",
      "Epoch 198/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1199 - mean_squared_error: 2309.1199\n",
      "Epoch 199/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1184 - mean_squared_error: 2309.1184\n",
      "Epoch 200/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1157 - mean_squared_error: 2309.1157\n",
      "Epoch 201/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1133 - mean_squared_error: 2309.1133\n",
      "Epoch 202/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1116 - mean_squared_error: 2309.1116\n",
      "Epoch 203/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1099 - mean_squared_error: 2309.1099\n",
      "Epoch 204/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1072 - mean_squared_error: 2309.1072\n",
      "Epoch 205/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1055 - mean_squared_error: 2309.1055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1035 - mean_squared_error: 2309.1035\n",
      "Epoch 207/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.1021 - mean_squared_error: 2309.1021\n",
      "Epoch 208/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.1003 - mean_squared_error: 2309.1003\n",
      "Epoch 209/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0989 - mean_squared_error: 2309.0989\n",
      "Epoch 210/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0972 - mean_squared_error: 2309.0972\n",
      "Epoch 211/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0959 - mean_squared_error: 2309.0959\n",
      "Epoch 212/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0940 - mean_squared_error: 2309.0940\n",
      "Epoch 213/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0925 - mean_squared_error: 2309.0925\n",
      "Epoch 214/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0913 - mean_squared_error: 2309.0913\n",
      "Epoch 215/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0906 - mean_squared_error: 2309.0906\n",
      "Epoch 216/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0894 - mean_squared_error: 2309.0894\n",
      "Epoch 217/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0881 - mean_squared_error: 2309.0881\n",
      "Epoch 218/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0874 - mean_squared_error: 2309.0874\n",
      "Epoch 219/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0857 - mean_squared_error: 2309.0857\n",
      "Epoch 220/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0850 - mean_squared_error: 2309.0850\n",
      "Epoch 221/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0847 - mean_squared_error: 2309.0847\n",
      "Epoch 222/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0828 - mean_squared_error: 2309.0828\n",
      "Epoch 223/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0823 - mean_squared_error: 2309.0823\n",
      "Epoch 224/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0808 - mean_squared_error: 2309.0808\n",
      "Epoch 225/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0801 - mean_squared_error: 2309.0801\n",
      "Epoch 226/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0793 - mean_squared_error: 2309.0793\n",
      "Epoch 227/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0791 - mean_squared_error: 2309.0791\n",
      "Epoch 228/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0784 - mean_squared_error: 2309.0784\n",
      "Epoch 229/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0774 - mean_squared_error: 2309.0774\n",
      "Epoch 230/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0767 - mean_squared_error: 2309.0767\n",
      "Epoch 231/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0764 - mean_squared_error: 2309.0764\n",
      "Epoch 232/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0762 - mean_squared_error: 2309.0762\n",
      "Epoch 233/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0745 - mean_squared_error: 2309.0745\n",
      "Epoch 234/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0745 - mean_squared_error: 2309.0745\n",
      "Epoch 235/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0737 - mean_squared_error: 2309.0737\n",
      "Epoch 236/400\n",
      "468/468 [==============================] - 0s 5us/sample - loss: 2309.0730 - mean_squared_error: 2309.0730\n",
      "Epoch 237/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0730 - mean_squared_error: 2309.0730\n",
      "Epoch 238/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0720 - mean_squared_error: 2309.0720\n",
      "Epoch 239/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0715 - mean_squared_error: 2309.0715\n",
      "Epoch 240/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0718 - mean_squared_error: 2309.0718\n",
      "Epoch 241/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0703 - mean_squared_error: 2309.0703\n",
      "Epoch 242/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0706 - mean_squared_error: 2309.0706\n",
      "Epoch 243/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0706 - mean_squared_error: 2309.0706\n",
      "Epoch 244/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0701 - mean_squared_error: 2309.0701\n",
      "Epoch 245/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0691 - mean_squared_error: 2309.0691\n",
      "Epoch 246/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0688 - mean_squared_error: 2309.0688\n",
      "Epoch 247/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0686 - mean_squared_error: 2309.0686\n",
      "Epoch 248/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0679 - mean_squared_error: 2309.0679\n",
      "Epoch 249/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0684 - mean_squared_error: 2309.0684\n",
      "Epoch 250/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0676 - mean_squared_error: 2309.0676\n",
      "Epoch 251/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0676 - mean_squared_error: 2309.0676\n",
      "Epoch 252/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0671 - mean_squared_error: 2309.0671\n",
      "Epoch 253/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0671 - mean_squared_error: 2309.0671\n",
      "Epoch 254/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0662 - mean_squared_error: 2309.0662\n",
      "Epoch 255/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0664 - mean_squared_error: 2309.0664\n",
      "Epoch 256/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0662 - mean_squared_error: 2309.0662\n",
      "Epoch 257/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0657 - mean_squared_error: 2309.0657\n",
      "Epoch 258/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0659 - mean_squared_error: 2309.0659\n",
      "Epoch 259/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0649 - mean_squared_error: 2309.0649\n",
      "Epoch 260/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0652 - mean_squared_error: 2309.0652\n",
      "Epoch 261/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0649 - mean_squared_error: 2309.0649\n",
      "Epoch 262/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0647 - mean_squared_error: 2309.0647\n",
      "Epoch 263/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0649 - mean_squared_error: 2309.0649\n",
      "Epoch 264/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0647 - mean_squared_error: 2309.0647\n",
      "Epoch 265/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0647 - mean_squared_error: 2309.0647\n",
      "Epoch 266/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0637 - mean_squared_error: 2309.0637\n",
      "Epoch 267/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0635 - mean_squared_error: 2309.0635\n",
      "Epoch 268/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0645 - mean_squared_error: 2309.0645\n",
      "Epoch 269/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0637 - mean_squared_error: 2309.0637\n",
      "Epoch 270/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0635 - mean_squared_error: 2309.0635\n",
      "Epoch 271/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0635 - mean_squared_error: 2309.0635\n",
      "Epoch 272/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0630 - mean_squared_error: 2309.0630\n",
      "Epoch 273/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0630 - mean_squared_error: 2309.0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0632 - mean_squared_error: 2309.0632\n",
      "Epoch 275/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0630 - mean_squared_error: 2309.0630\n",
      "Epoch 276/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 277/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0632 - mean_squared_error: 2309.0632\n",
      "Epoch 278/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0627 - mean_squared_error: 2309.0627\n",
      "Epoch 279/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 280/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0623 - mean_squared_error: 2309.0623\n",
      "Epoch 281/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 282/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0627 - mean_squared_error: 2309.0627\n",
      "Epoch 283/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 284/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0623 - mean_squared_error: 2309.0623\n",
      "Epoch 285/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 286/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0625 - mean_squared_error: 2309.0625\n",
      "Epoch 287/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0620 - mean_squared_error: 2309.0620\n",
      "Epoch 288/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 289/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 290/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 291/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0620 - mean_squared_error: 2309.0620\n",
      "Epoch 292/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 293/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 294/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 295/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 296/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 297/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 298/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 299/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 300/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0620 - mean_squared_error: 2309.0620\n",
      "Epoch 301/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 302/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 303/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 304/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 305/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 306/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0618 - mean_squared_error: 2309.0618\n",
      "Epoch 307/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 308/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 309/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 310/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 311/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 312/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 313/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 314/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 315/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 316/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 317/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 318/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0613 - mean_squared_error: 2309.0613\n",
      "Epoch 319/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 320/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 321/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 322/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 323/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 324/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 325/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 326/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 327/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 328/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 329/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 330/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 331/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 332/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 333/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 334/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 335/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 336/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 337/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 338/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 339/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 340/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 341/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 343/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 344/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 345/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 346/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 347/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 348/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 349/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 350/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 351/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 352/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 353/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 354/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 355/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 356/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 357/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0608 - mean_squared_error: 2309.0608\n",
      "Epoch 358/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 359/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 360/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 361/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 362/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 363/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 364/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 365/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 366/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 367/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 368/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 369/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 370/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 371/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 372/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 373/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0615 - mean_squared_error: 2309.0615\n",
      "Epoch 374/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 375/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 376/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 377/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 378/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 379/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 380/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 381/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 382/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 383/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 384/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 385/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 386/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0605 - mean_squared_error: 2309.0605\n",
      "Epoch 387/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 388/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 389/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 390/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 391/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 392/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 393/400\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 394/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 395/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0598 - mean_squared_error: 2309.0598\n",
      "Epoch 396/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 397/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 398/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0601 - mean_squared_error: 2309.0601\n",
      "Epoch 399/400\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n",
      "Epoch 400/400\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 2309.0603 - mean_squared_error: 2309.0603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b46f44c828>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: build model\n",
    "model2 = Sequential()\n",
    "#input layer\n",
    "model2.add(Dense(20, input_dim = 31, activation = 'sigmoid'))\n",
    "#hidden layers\n",
    "model2.add(Dense(10, activation = 'relu'))\n",
    "model2.add(Dense(5, activation = 'relu'))\n",
    "#output layer\n",
    "model2.add(Dense(1))\n",
    "#step 2: compile the model\n",
    "model2.compile(loss= 'mse' , optimizer = 'sgd',metrics = ['mse'] )\n",
    "#step 3: train the model\n",
    "model2.fit(X_train, y_train, epochs = 400, batch_size = 468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 0s 200us/sample - loss: 2309.0604 - mean_squared_error: 2309.0603\n",
      "157/157 [==============================] - 0s 38us/sample - loss: 447.4989 - mean_squared_error: 447.4988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[447.49887235605036, 447.49884]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_train, y_train)\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squred training score for multiple layer neutral network: -7.115872335816675e-09\n",
      "R_squred testing score for multiple layer neutral network: -0.008897861863102152\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model2.predict(X_train)\n",
    "y_pred_test = model2.predict(X_test)\n",
    "print('R_squred training score for multiple layer neutral network: {}'.format(r2_score(y_train, y_pred_train)))\n",
    "print('R_squred testing score for multiple layer neutral network: {}'.format(r2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Audit_Risk', 'Risk'], axis = 1)\n",
    "y = df['Risk']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard voting and soft voting from KNN classifier, logit regression and svc with rbf kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9936305732484076\n",
      "KNeighborsClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "### Hard voting \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train, y_train)\n",
    "knn_clf = KNeighborsClassifier(3)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "svm_clf = SVC(kernel='rbf', C=10, gamma=1, probability = True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "Hvoting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)], voting='hard')\n",
    "Hvoting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, knn_clf, svm_clf, Hvoting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9936305732484076\n",
      "KNeighborsClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "### Soft voting \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train, y_train)\n",
    "knn_clf = KNeighborsClassifier(3)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "svm_clf = SVC(kernel='rbf', C=10, gamma=1, probability = True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "Svoting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)], voting='soft')\n",
    "Svoting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, knn_clf, svm_clf, Svoting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting on KNN Classifier and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Bagging on KNN Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "\n",
    "knn_clf = KNeighborsClassifier(3) # From project1 we know that the best parameter for KNN classifier is n=3.\n",
    "bag_knn = BaggingClassifier(knn_clf, bootstrap=True, random_state=0)\n",
    "grid_search = GridSearchCV(bag_knn, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for bagging knn classifier with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "bag_knn1 = BaggingClassifier(knn_clf, n_estimators=100, bootstrap=True, random_state=0)\n",
    "bag_knn1.fit(X_train, y_train)\n",
    "y_test_pred = bag_knn1.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score for bagging knn classifier with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Bagging on Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "dtree_clf = DecisionTreeClassifier()\n",
    "bag_dtree = BaggingClassifier(dtree_clf, bootstrap=True, random_state=0)\n",
    "grid_search = GridSearchCV(bag_dtree, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for bagging decision tree with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "bag_dtree1 = BaggingClassifier(dtree_clf, n_estimators=100, bootstrap=True, random_state=0)\n",
    "bag_dtree1.fit(X_train, y_train)\n",
    "y_test_pred = bag_dtree1.predict(X_test)\n",
    "print(\"Accuracy score for bagging decision tree with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Pasting on KNN Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "\n",
    "knn_clf = KNeighborsClassifier(3) # From project1 we know that the best parameter for KNN classifier is n=3.\n",
    "bag_knn = BaggingClassifier(knn_clf, bootstrap=False, random_state=0)\n",
    "grid_search = GridSearchCV(bag_knn, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for pasting knn classifier with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "past_knn1 = BaggingClassifier(knn_clf, n_estimators=100, bootstrap=False, random_state=0)\n",
    "past_knn1.fit(X_train, y_train)\n",
    "y_test_pred = past_knn1.predict(X_test)\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Accuracy score for pasting knn classifier with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Pasting on Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "dtree_clf = DecisionTreeClassifier()\n",
    "bag_dtree = BaggingClassifier(dtree_clf, bootstrap=False, random_state=0)\n",
    "grid_search = GridSearchCV(bag_dtree, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for pasting decision tree with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "bag_dtree1 = BaggingClassifier(dtree_clf, n_estimators=100, bootstrap=False, random_state=0)\n",
    "bag_dtree1.fit(X_train, y_train)\n",
    "y_test_pred = bag_dtree1.predict(X_test)\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Accuracy score for pasting decision tree with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost boosting on Decision Tree and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Adaboost on Decision Tree Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_clf = DecisionTreeClassifier()\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "ada_dtree = AdaBoostClassifier(dtree_clf, random_state=0)\n",
    "grid_search = GridSearchCV(ada_dtree, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for adaboost decision tree classifier with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "ada_dtree1 = AdaBoostClassifier(dtree_clf, n_estimators=100, learning_rate=0.2, random_state=0)\n",
    "ada_dtree1.fit(X_train, y_train)\n",
    "y_test_pred = ada_dtree1.predict(X_test)\n",
    "print(\"Accuracy score for adaboost decision tree classifier with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Adaboost on Logit Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg_clf = LogisticRegression(penalty = 'l2', C = 0.1) # From project1 we know that the best pramaters for logit regression is C=0.1 and penalty=l2.\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "ada_lg = AdaBoostClassifier(lg_clf, random_state=0)\n",
    "grid_search = GridSearchCV(ada_lg, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for adaboost logit regression with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "ada_lg1 = AdaBoostClassifier(lg_clf, n_estimators=100, learning_rate=0.2, random_state=0)\n",
    "ada_lg1.fit(X_train, y_train)\n",
    "y_test_pred = ada_lg1.predict(X_test)\n",
    "print(\"Accuracy score for adaboost logit regression with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model(Decision Tree Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param = {\"n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.2, 0.4, 0.6, 0.8, 1]}\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(gbrt, param, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for gradient boosting classifier with best parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "gbrt_clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, random_state=0)\n",
    "gbrt_clf1.fit(X_train, y_train)\n",
    "y_test_pred = gbrt_clf1.predict(X_test)\n",
    "print(\"Accuracy score for gradient boosting classifier with best parameters: {}\".format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Classification with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are using the X_train_reduced and X_test_reduced from the regression since both classification and regression share the same feature data set. And we already found the best number of dimension with grid search from previous work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for KNN Classifier after PCA: 1.0\n"
     ]
    }
   ],
   "source": [
    "# KNN classifier after PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf3 = KNeighborsClassifier(3) # From project1 we know the best parameter for knn classifier is k=3.\n",
    "knn_clf3.fit(X_train_reduced, y_train)\n",
    "y_pred = knn_clf3.predict(X_test_reduced)\n",
    "print(\"Accuracy score for KNN Classifier after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for logit regression after PCA: 0.9554140127388535\n"
     ]
    }
   ],
   "source": [
    "# Logic regressoin after PCA\n",
    "lg_clf1 = LogisticRegression(C=0.1, penalty='l2') # From project1 we know the best parameter for logit regression is C=0.1, penalty=l2.\n",
    "lg_clf1.fit(X_train_reduced, y_train)\n",
    "y_pred = lg_clf1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for logit regression after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for LinearSVC after PCA: 1.0\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC after PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "SVC1 = LinearSVC(C=1) # From project1 we know the best parameter for LinearSVC is C=1.\n",
    "SVC1.fit(X_train_reduced, y_train)\n",
    "y_pred = SVC1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for LinearSVC after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC with rbf kernel after PCA: 1.0\n"
     ]
    }
   ],
   "source": [
    "# SVC with rbf kernel after PCA\n",
    "from sklearn.svm import SVC\n",
    "SVC_rbf1 = SVC(kernel='rbf', C=10, gamma=1) # From project1 we know the best parameter for SVC with rbf kernel is C=10, gamma=1.\n",
    "SVC_rbf1.fit(X_train_reduced, y_train)\n",
    "y_pred = SVC_rbf1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for SVC with rbf kernel after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC with poly kernel after PCA: 0.9617834394904459\n"
     ]
    }
   ],
   "source": [
    "# SVC with poly kernel after PCA\n",
    "from sklearn.svm import SVC\n",
    "SVC_poly1 = SVC(kernel='poly', C=100, gamma=0.1) # From project1 we know the best parameter for SVC with rbf kernel is C=100, gamma=0.1.\n",
    "SVC_poly1.fit(X_train_reduced, y_train)\n",
    "y_pred = SVC_poly1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for SVC with poly kernel after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC with linear kernel after PCA: 0.9872611464968153\n"
     ]
    }
   ],
   "source": [
    "# SVC with linear kernel after PCA\n",
    "from sklearn.svm import SVC\n",
    "SVC_ln1 = SVC(kernel='linear', C=0.1, gamma=0.001) # From project1 we know the best parameter for SVC with linear kernel is C=0.1, gamma=0.001.\n",
    "SVC_ln1.fit(X_train_reduced, y_train)\n",
    "y_pred = SVC_ln1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for SVC with linear kernel after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for decision tree after PCA: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Decision tree after PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree1 = DecisionTreeClassifier() # From project1 we know the best parameter for SVC with linear kernel is C=0.1, gamma=0.001.\n",
    "dtree1.fit(X_train_reduced, y_train)\n",
    "y_pred = dtree1.predict(X_test_reduced)\n",
    "print(\"Accuracy score for decision tree after PCA: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning: Simple Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 31)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "468/468 [==============================] - 0s 475us/sample - loss: 0.6896 - acc: 0.5620\n",
      "Epoch 2/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6866 - acc: 0.5876\n",
      "Epoch 3/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6835 - acc: 0.6047\n",
      "Epoch 4/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6805 - acc: 0.6090\n",
      "Epoch 5/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6776 - acc: 0.6282\n",
      "Epoch 6/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6746 - acc: 0.6410\n",
      "Epoch 7/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6716 - acc: 0.6581\n",
      "Epoch 8/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6686 - acc: 0.6624\n",
      "Epoch 9/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6657 - acc: 0.6624\n",
      "Epoch 10/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6628 - acc: 0.6859\n",
      "Epoch 11/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6599 - acc: 0.6944\n",
      "Epoch 12/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6569 - acc: 0.7030\n",
      "Epoch 13/40\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 0.6540 - acc: 0.7201\n",
      "Epoch 14/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6511 - acc: 0.7350\n",
      "Epoch 15/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6482 - acc: 0.7457\n",
      "Epoch 16/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6453 - acc: 0.7607\n",
      "Epoch 17/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6424 - acc: 0.8013\n",
      "Epoch 18/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6394 - acc: 0.8098\n",
      "Epoch 19/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6365 - acc: 0.8141\n",
      "Epoch 20/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6335 - acc: 0.8141\n",
      "Epoch 21/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6306 - acc: 0.8419\n",
      "Epoch 22/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6276 - acc: 0.8547\n",
      "Epoch 23/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6247 - acc: 0.8761\n",
      "Epoch 24/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6217 - acc: 0.8846\n",
      "Epoch 25/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6188 - acc: 0.8953\n",
      "Epoch 26/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6158 - acc: 0.9081\n",
      "Epoch 27/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6129 - acc: 0.9252\n",
      "Epoch 28/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6099 - acc: 0.9274\n",
      "Epoch 29/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6070 - acc: 0.9402\n",
      "Epoch 30/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6040 - acc: 0.9487\n",
      "Epoch 31/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6011 - acc: 0.9509\n",
      "Epoch 32/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5981 - acc: 0.9509\n",
      "Epoch 33/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5952 - acc: 0.9530\n",
      "Epoch 34/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5923 - acc: 0.9530\n",
      "Epoch 35/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5894 - acc: 0.9551\n",
      "Epoch 36/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5865 - acc: 0.9594\n",
      "Epoch 37/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5836 - acc: 0.9615\n",
      "Epoch 38/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5807 - acc: 0.9679\n",
      "Epoch 39/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5778 - acc: 0.9701\n",
      "Epoch 40/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5750 - acc: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b4737a2ba8>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: build model\n",
    "model1 = Sequential()\n",
    "#input layer\n",
    "model1.add(Dense(10, input_dim = 31, activation = 'relu'))\n",
    "#output layer\n",
    "model1.add(Dense(1, activation = 'sigmoid'))\n",
    "#step 2: make computational graph - compile\n",
    "model1.compile(loss= 'binary_crossentropy' , optimizer = 'adam',metrics = ['accuracy'] )\n",
    "#step 3: train the model - fit\n",
    "model1.fit(X_train, y_train, epochs = 40, batch_size = 468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 0s 222us/sample - loss: 0.5721 - acc: 0.9722\n",
      "157/157 [==============================] - 0s 38us/sample - loss: 0.5755 - acc: 0.9554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5754925566873733, 0.955414]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_train, y_train)\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for simple neutral network: 0.9722222222222222\n",
      "Testing accuracy score for simple neutral network: 0.9554140127388535\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model1.predict(X_train)\n",
    "y_pred_test = model1.predict(X_test)\n",
    "y_train_pred = np.where(y_pred_train >= 0.5, 1, 0)\n",
    "y_test_pred = np.where(y_pred_test >= 0.5, 1, 0)\n",
    "print('Training accuracy score for simple neutral network: {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Testing accuracy score for simple neutral network: {}'.format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning: Neural Networks with Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "468/468 [==============================] - 0s 714us/sample - loss: 0.6878 - acc: 0.6261\n",
      "Epoch 2/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6864 - acc: 0.6474\n",
      "Epoch 3/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6848 - acc: 0.6688\n",
      "Epoch 4/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6832 - acc: 0.6752\n",
      "Epoch 5/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6815 - acc: 0.6923\n",
      "Epoch 6/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6795 - acc: 0.7265\n",
      "Epoch 7/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6774 - acc: 0.7479\n",
      "Epoch 8/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6753 - acc: 0.7585\n",
      "Epoch 9/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6730 - acc: 0.7863\n",
      "Epoch 10/40\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 0.6705 - acc: 0.7842\n",
      "Epoch 11/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6678 - acc: 0.7799\n",
      "Epoch 12/40\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 0.6651 - acc: 0.7927\n",
      "Epoch 13/40\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 0.6621 - acc: 0.8056\n",
      "Epoch 14/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6590 - acc: 0.8056\n",
      "Epoch 15/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6558 - acc: 0.8098\n",
      "Epoch 16/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6525 - acc: 0.8184\n",
      "Epoch 17/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6491 - acc: 0.8226\n",
      "Epoch 18/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6455 - acc: 0.8312\n",
      "Epoch 19/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6419 - acc: 0.8333\n",
      "Epoch 20/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6382 - acc: 0.8226\n",
      "Epoch 21/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6345 - acc: 0.8312\n",
      "Epoch 22/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6307 - acc: 0.8248\n",
      "Epoch 23/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6270 - acc: 0.8226\n",
      "Epoch 24/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6232 - acc: 0.8205\n",
      "Epoch 25/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6194 - acc: 0.8226\n",
      "Epoch 26/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6155 - acc: 0.8248\n",
      "Epoch 27/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6117 - acc: 0.8397\n",
      "Epoch 28/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.6078 - acc: 0.8397\n",
      "Epoch 29/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.6038 - acc: 0.8526\n",
      "Epoch 30/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5999 - acc: 0.8697\n",
      "Epoch 31/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5960 - acc: 0.8739\n",
      "Epoch 32/40\n",
      "468/468 [==============================] - 0s 9us/sample - loss: 0.5921 - acc: 0.8761\n",
      "Epoch 33/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5882 - acc: 0.8846\n",
      "Epoch 34/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5843 - acc: 0.8846\n",
      "Epoch 35/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5803 - acc: 0.8825\n",
      "Epoch 36/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5764 - acc: 0.8846\n",
      "Epoch 37/40\n",
      "468/468 [==============================] - 0s 7us/sample - loss: 0.5724 - acc: 0.8910\n",
      "Epoch 38/40\n",
      "468/468 [==============================] - 0s 4us/sample - loss: 0.5685 - acc: 0.8974\n",
      "Epoch 39/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5645 - acc: 0.8996\n",
      "Epoch 40/40\n",
      "468/468 [==============================] - 0s 6us/sample - loss: 0.5605 - acc: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b474101208>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: build model\n",
    "model2 = Sequential()\n",
    "#input layer\n",
    "model2.add(Dense(20, input_dim = 31, activation = 'relu'))\n",
    "#hidden layers\n",
    "model2.add(Dense(10, activation = 'relu'))\n",
    "model2.add(Dense(5, activation = 'relu'))\n",
    "#output layer\n",
    "model2.add(Dense(1, activation = 'sigmoid'))\n",
    "#step 2: compile the model\n",
    "model2.compile(loss= 'binary_crossentropy' , optimizer = 'adam',metrics = ['accuracy'] )\n",
    "#step 3: train the model\n",
    "model2.fit(X_train, y_train, epochs = 40, batch_size = 468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 0s 336us/sample - loss: 0.5565 - acc: 0.9060\n",
      "157/157 [==============================] - 0s 38us/sample - loss: 0.5520 - acc: 0.9172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5519759844822488, 0.91719747]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_train, y_train)\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for neutral network with multiple layers: 0.905982905982906\n",
      "Testing accuracy score for neutral network with multiple layers: 0.9171974522292994\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model2.predict(X_train)\n",
    "y_pred_test = model2.predict(X_test)\n",
    "y_train_pred = np.where(y_pred_train >= 0.5, 1, 0)\n",
    "y_test_pred = np.where(y_pred_test >= 0.5, 1, 0)\n",
    "print('Training accuracy score for neutral network with multiple layers: {}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('Testing accuracy score for neutral network with multiple layers: {}'.format(accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>Testing R_Square</th>\n",
       "      <th>Testing R_Squre after PCA</th>\n",
       "      <th>Testing R_Squre after Bagging</th>\n",
       "      <th>Testing R_Squre after Pasting</th>\n",
       "      <th>Testing R_Square after Adaboost boosting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn Regression</td>\n",
       "      <td>n_neighbors=3</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>alpha=10</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.995</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>alpha=1</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polynomial Regression</td>\n",
       "      <td>degree=2</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVR</td>\n",
       "      <td>C=100</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR(kenerl='rbf')</td>\n",
       "      <td>C=100, gamma=0.1</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVR(kenerl='poly')</td>\n",
       "      <td>C=1, gamma=10</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.975</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVR(kenerl='linear')</td>\n",
       "      <td>C=100, gamma=0.001</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gridient Boosting Regressor(on Decision Tree R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Simple Neural Network</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ML Neural Network</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Model Name     Best Parameters  \\\n",
       "0                                      Knn Regression       n_neighbors=3   \n",
       "1                                   Linear Regression                 NaN   \n",
       "2                                    Ridge Regression            alpha=10   \n",
       "3                                    Lasso Regression             alpha=1   \n",
       "4                               Polynomial Regression            degree=2   \n",
       "5                                           LinearSVR               C=100   \n",
       "6                                   SVR(kenerl='rbf')    C=100, gamma=0.1   \n",
       "7                                  SVR(kenerl='poly')       C=1, gamma=10   \n",
       "8                                SVR(kenerl='linear')  C=100, gamma=0.001   \n",
       "9   Gridient Boosting Regressor(on Decision Tree R...                 NaN   \n",
       "10                              Simple Neural Network                 NaN   \n",
       "11                                  ML Neural Network                 NaN   \n",
       "\n",
       "    Training Score  Testing Score  Testing R_Square  \\\n",
       "0            0.352          0.444             0.626   \n",
       "1           -1.593          0.697             0.444   \n",
       "2            0.404         -0.995             0.662   \n",
       "3            0.525         -0.563             0.697   \n",
       "4            0.911          0.963             0.999   \n",
       "5            0.644          0.452             0.687   \n",
       "6            0.696          0.369             0.797   \n",
       "7            0.945          0.829             0.975   \n",
       "8            0.646          0.436             0.689   \n",
       "9              NaN            NaN             0.898   \n",
       "10             NaN            NaN             0.872   \n",
       "11             NaN            NaN            -0.009   \n",
       "\n",
       "    Testing R_Squre after PCA  Testing R_Squre after Bagging  \\\n",
       "0                       0.564                            NaN   \n",
       "1                       0.681                          0.604   \n",
       "2                       0.680                            NaN   \n",
       "3                       0.681                            NaN   \n",
       "4                      -0.028                          0.998   \n",
       "5                       0.679                            NaN   \n",
       "6                       0.778                            NaN   \n",
       "7                      -0.459                            NaN   \n",
       "8                      -0.104                            NaN   \n",
       "9                         NaN                            NaN   \n",
       "10                        NaN                            NaN   \n",
       "11                        NaN                            NaN   \n",
       "\n",
       "    Testing R_Squre after Pasting  Testing R_Square after Adaboost boosting  \n",
       "0                             NaN                                       NaN  \n",
       "1                           0.444                                     0.318  \n",
       "2                             NaN                                       NaN  \n",
       "3                             NaN                                       NaN  \n",
       "4                           0.999                                     0.999  \n",
       "5                             NaN                                       NaN  \n",
       "6                             NaN                                       NaN  \n",
       "7                             NaN                                       NaN  \n",
       "8                             NaN                                       NaN  \n",
       "9                             NaN                                       NaN  \n",
       "10                            NaN                                       NaN  \n",
       "11                            NaN                                       NaN  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regression = pd.read_csv('Regression_Results.csv')\n",
    "Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can see that most models have a lower R_squared testing score after dimension reduction with PCA, indicating a better model performance without PCA. Only linear regression and ridge regression have improved testing R_squred scores. The linear regression's testing R_square improved from 0.444 to 0.681 after PCA. The ridge regression's testing R_square improved from 0.662 to 0.680.\n",
    "\n",
    "In terms of ensembling, only bagging on linear regressoin improved the model performace(0.604 to 0.444). None of the ensembling methods improved the performance of polynomial regression.\n",
    "\n",
    "For the deep learning models, the netural network without hidden layer has a better performance with a testing R_square of 0.872.\n",
    "\n",
    "Among all of these models, polynomial regression with two degree without PCA has the best performace, with a testing R_squared score of 0.999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Testing Score</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Testing Accuracy after PCA</th>\n",
       "      <th>Testing Accuracy after Bagging</th>\n",
       "      <th>Testing Accuracy after Pasting</th>\n",
       "      <th>Testing Accuracy after Adaboost boosting</th>\n",
       "      <th>Testing Accuracy after  GDB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn Classifier</td>\n",
       "      <td>n_neighbors=3</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logic Regression</td>\n",
       "      <td>C=0.1, penalty=l2</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>C=1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC(kenerl='rbf')</td>\n",
       "      <td>C=10, gamma=1</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(kenerl='poly')</td>\n",
       "      <td>C=100, gamma=0.1</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(kenerl='linear')</td>\n",
       "      <td>C=0.1, gamma=0.001</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Simple Neural Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ML Neural Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model Name     Best Parameters  Training Score  Testing Score  \\\n",
       "0          Knn Classifier       n_neighbors=3           0.998          1.000   \n",
       "1        Logic Regression   C=0.1, penalty=l2           0.987          0.981   \n",
       "2               LinearSVC                 C=1           1.000          1.000   \n",
       "3       SVC(kenerl='rbf')       C=10, gamma=1           0.998          1.000   \n",
       "4      SVC(kenerl='poly')    C=100, gamma=0.1           0.989          0.968   \n",
       "5    SVC(kenerl='linear')  C=0.1, gamma=0.001           0.991          0.981   \n",
       "6           Decision Tree                 NaN           1.000          1.000   \n",
       "7  Simple Neural Networks                 NaN           0.912            NaN   \n",
       "8      ML Neural Networks                 NaN           1.000            NaN   \n",
       "\n",
       "   Testing Accuracy  Testing Accuracy after PCA  \\\n",
       "0             1.000                       1.000   \n",
       "1             0.981                       0.955   \n",
       "2             1.000                       1.000   \n",
       "3             1.000                       1.000   \n",
       "4             0.981                       0.963   \n",
       "5             0.987                       0.987   \n",
       "6             1.000                       1.000   \n",
       "7               NaN                         NaN   \n",
       "8               NaN                         NaN   \n",
       "\n",
       "   Testing Accuracy after Bagging  Testing Accuracy after Pasting  \\\n",
       "0                             1.0                             1.0   \n",
       "1                             NaN                             NaN   \n",
       "2                             NaN                             NaN   \n",
       "3                             NaN                             NaN   \n",
       "4                             NaN                             NaN   \n",
       "5                             NaN                             NaN   \n",
       "6                             1.0                             1.0   \n",
       "7                             NaN                             NaN   \n",
       "8                             NaN                             NaN   \n",
       "\n",
       "   Testing Accuracy after Adaboost boosting  Testing Accuracy after  GDB  \n",
       "0                                       NaN                          NaN  \n",
       "1                                       1.0                          NaN  \n",
       "2                                       NaN                          NaN  \n",
       "3                                       NaN                          NaN  \n",
       "4                                       NaN                          NaN  \n",
       "5                                       NaN                          NaN  \n",
       "6                                       1.0                          1.0  \n",
       "7                                       NaN                          NaN  \n",
       "8                                       NaN                          NaN  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classification = pd.read_csv('Classification_Results.csv')\n",
    "Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can see that none of the classification models got improved accuracy score after PCA, indicating a better model performance without PCA. There are 4 models have a 100% testing accuracy with and without PCA, including KNeighbor Classifier, LinearSVC, SVC with rbf kernel and decision tree. \n",
    "\n",
    "In terms of ensembling, none of the applied ensembling methods made any difference on the model performance.\n",
    "\n",
    "For the deep learning models, the netural network without hidden layer has a testing accuracy of 0.912, less that the netural network with multiple layers with a testing accuracy of 1. In this case, the ML neural network performed better. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
